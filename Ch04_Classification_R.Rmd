---
title: "Classification"
date: "2023-01-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.align = "center")
pacman::p_load(arm,
       car
      , dplyr
      , ggplot2
      , ggExtra
      , reshape2
      , corrplot
      , RColorBrewer
      , lubridate
      , mlr3
      , mlr3learners
      , tidymodels
      , caret
      )
```

```{css,echo=FALSE}
.btn {
    border-width: 0 0px 0px 0px;
    font-weight: normal;
    text-transform: ;
}

.btn-default {
    color: #2ecc71;
    background-color: #ffffff;
    border-color: #ffffff;
}
```

```{r,echo=FALSE}
# Global parameter
show_code <- TRUE
```
# Class Workbook {.tabset .tabset-fade .tabset-pills}

## In class activity

### COVID-19 Survival in Mexico

Your task is determining whether a patient will survive COVID-19 in Mexico.
The original dataset comes from the [Mexican government](https://datos.gob.mx/busca/dataset/informacion-referente-a-casos-covid-19-en-mexico).  This data is a version downloaded from [Kaggle](https://www.kaggle.com/datasets/meirnizri/covid19-dataset?resource=download).  The raw dataset consists of 21 unique features and 1,048,576 unique patients. In the Boolean features, 1 means "yes" and 2 means "no". values as 97 and 99 are missing data.

- sex: 1 for female and 2 for male.
- age: of the patient.
- classification: COVID test findings. Values 1-3 mean that the patient was diagnosed with COVID in different degrees. 4 or higher means that the patient is not a carrier of COVID or that the test is inconclusive.
- patient type: type of care the patient received in the unit. 1 for returned home and 2 for hospitalization.
- pneumonia: whether the patient already have air sacs inflammation or not.
- pregnancy: whether the patient is pregnant or not.
- diabetes: whether the patient has diabetes or not.
- copd: Indicates whether the patient has Chronic obstructive pulmonary disease or not.
- asthma: whether the patient has asthma or not.
- inmsupr: whether the patient is immunosuppressed or not.
- hypertension: whether the patient has hypertension or not.
- cardiovascular: whether the patient has heart or blood vessels related disease.
- renal chronic: whether the patient has chronic renal disease or not.
- other disease: whether the patient has other disease or not.
- obesity: whether the patient is obese or not.
- tobacco: whether the patient is a tobacco user.
- usmr: Indicates whether the patient treated medical units of the first, second or third level.
- medical unit: type of institution of the National Health System that provided the care.
- intubed: whether the patient was connected to the ventilator.
- icu: Indicates whether the patient had been admitted to an Intensive Care Unit.
- date died: If the patient died indicate the date of death, and 9999-99-99 otherwise.

```{r,echo=show_code}
Covid_Data<- read.csv("Covid_Data.csv")
```

Creating survival variable
```{r,echo=show_code,cache=TRUE}
Covid_Data$SURVIVED <- factor(1*(Covid_Data$DATE_DIED!="9999-99-99")+1,
                              levels=c(1,2),labels=c("yes","no"))
                              # 1 is yes and 2 is no in concordance with the original data
```

Gender Factoring
```{r,echo=show_code}
Covid_Data$SEX <- factor(Covid_Data$SEX ,levels=c(1,2),labels=c("female","male"))
Covid_Data$FEMALE <- 1*(Covid_Data$SEX=="female")
```

Missing data are coded as 99 or 97, we need to change them to NA.  However we need to know which variable to do so before making the change.

```{r,echo=show_code}
colSums(Covid_Data==99)
colSums(Covid_Data==97)

## Recode missing data as NA ##
Covid_Data$INTUBED  <- ifelse( Covid_Data$INTUBED  <3, Covid_Data$INTUBED, NA)
Covid_Data$PNEUMONIA<- ifelse( Covid_Data$PNEUMONIA<3, Covid_Data$PNEUMONIA, NA)
Covid_Data$PREGNANT <- ifelse( Covid_Data$PREGNANT <3, Covid_Data$PREGNANT, NA)
Covid_Data$ICU      <- ifelse( Covid_Data$ICU      <3, Covid_Data$ICU, NA)

## Factor the variables

Covid_Data$INTUBED  <- factor( Covid_Data$INTUBED   , levels=c(1,2),labels=c("yes","no"))
Covid_Data$PNEUMONIA<- factor( Covid_Data$PNEUMONIA , levels=c(1,2),labels=c("yes","no"))
Covid_Data$PREGNANT <- factor( Covid_Data$PREGNANT  , levels=c(1,2),labels=c("yes","no"))
Covid_Data$ICU      <- factor( Covid_Data$ICU       , levels=c(1,2),labels=c("yes","no"))

Covid_Data$PREGNANT[Covid_Data$FEMALE ==0 & is.na(Covid_Data$PREGNANT)] <-"no"
```

### Looking at AGE

You can see a clear trend by age.
```{r,echo=show_code}
ggplot(Covid_Data)+geom_bar()+aes(x=AGE,fill=SURVIVED)
ggplot(Covid_Data)+geom_bar(position="fill")+aes(x=AGE,fill=SURVIVED)
table(Covid_Data$AGE)
```

Since there are very few people above 100, we will look at people above 100 as 100.  This is to make our problem easier, but it also tries to address the limit of what we can say.
```{r,echo=show_code}
Covid_Data$AGE_TRUNK <- ifelse(Covid_Data$AGE<101,Covid_Data$AGE,100)
ggplot(Covid_Data)+geom_bar(position="fill")+aes(x=AGE_TRUNK,fill=SURVIVED)
```

### Looking at Gender

Distribution by gender and age shows differing rate for male and female that needs to be accounted for.
```{r,echo=show_code}
ggplot(Covid_Data)+geom_bar(position="fill")+aes(x=AGE_TRUNK,fill=SURVIVED)+facet_wrap(~SEX)
```

### An initial look.

We fit a model with age and gender.
```{r,echo=show_code}
Covid_Data$SURVIVED_bin <-1*(Covid_Data$SURVIVED=="yes")
glm_fit<- glm(SURVIVED_bin~AGE_TRUNK*SEX,family=binomial,data= Covid_Data)
glm_fit$converged
```
the model converges.

```{r,echo=show_code}
summary(glm_fit)
```

Confusion matrix with a cutoff of 0.5 looks like
```{r,echo=show_code}
logit_P = predict(glm_fit,Covid_Data,type="response")
logit_Pred <- ifelse(logit_P > 0.5,1,0) # Probability check
CM= table(Covid_Data$SURVIVED_bin, logit_Pred)
print(CM)
```

The ROC Curve looks like
```{r,echo=show_code}
library(pROC)
roc_score=roc(Covid_Data$SURVIVED_bin, logit_P) #AUC score
plot(roc_score ,main ="ROC curve -- Logistic Regression ")
### Using ROCR
# library(ROCR)
# pred_res = prediction(logit_P, Covid_Data$SURVIVED_bin)
# perf = performance(pred_res, "acc")
# plot(perf)
# roc = performance(pred_res,"tpr","fpr")
# plot(roc, colorize = T, lwd = 2)
# abline(a = 0, b = 1) 
```
It seems to be doing fairly well.

Let's look at a decile plot.  Decile plot looks at prediction accuracy by the predicted probability.  Since seeing the lower probability is easier, we will look at the predicted death probability.  You can read about Decile plots here: (https://select-statistics.co.uk/blog/cumulative-gains-and-lift-curves-measuring-the-performance-of-a-marketing-campaign/)
```{r,echo=show_code}
lift <- function(depvar, predcol, groups=10) {
  if(is.factor(depvar)) depvar <- as.integer(as.character(depvar))
  if(is.factor(predcol)) predcol <- as.integer(as.character(predcol))
  helper = data.frame(cbind(depvar, predcol))
  helper[,"bucket"] = ntile(-helper[,"predcol"], groups)
  gaintable = helper %>% group_by(bucket)  %>%
    summarise_at(vars(depvar), list(total = ~n(),
    totalresp=~sum(., na.rm = TRUE))) %>%
    mutate(Cumresp = cumsum(totalresp),
    Gain=Cumresp/sum(totalresp)*100,
    Cumlift=Gain/(bucket*(100/groups)))
  return(gaintable)
}

death = 1*(Covid_Data$SURVIVED_bin==0)
revP =1-logit_P
dt = lift(death, revP, groups = 10)
barplot(dt$totalresp/dt$total,  ylab="Decile", xlab="Bucket")
abline(h=mean(death ),lty=2,col="red")
```

This plot shows that the model does capture the riskiest patients well in the first three bins.

However, when you look at the residual, there are clear signs that the model does not fit very well.  

```{r,echo=show_code}


arm::binnedplot(predict(glm_fit,type="response"),resid(glm_fit,type="response" ))
```
The model is not capturing the lower-risk patients; there is nonlinearity, and the very high-risk patients have some problems, too.  Given the EDA plots we've seen so far, these should not be a surprise.  So, what can we do to improve the performance of the model?

### In-class activity

Your goal is to use 30% of the patients as your training data and predict the outcome of the remaining patients.  It's important to note that you also need to get a prediction for patients with NA values. 
```{r,echo=show_code}
size<-floor(dim(Covid_Data)[1]*0.3)
set.seed(1257)
samp_idx<-sample(1:dim(Covid_Data)[1],size,FALSE)
Train_COVID<- Covid_Data[samp_idx,]
Test_COVID <- Covid_Data[-samp_idx,]
saveRDS(Train_COVID,"Train_COVID.rds")
saveRDS(Test_COVID, "Test_COVID.rds")
# Sanity check
table(Train_COVID$SURVIVED)/nrow(Train_COVID)
table(Test_COVID$SURVIVED)/nrow(Test_COVID)
```


Q1. What metric will you use? Why? 

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

Q2. What was your best performing model?  

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

Q3. Explain how you got the model in detail.


Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

Q4. Based on your model, what factor seems to be important in deciding if a patient survives or not.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

## Problem Set

### Auto Data

In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.

(a) Create a binary variable, `mpg01`, that contains a `1` if mpg contains a value above its median, and a `0` if mpg contains a value below its median. You can compute the median using the `median()` function. Note you may find it helpful to use the `data.frame()` function to create a single data set containing both mpg01 and the other `Auto` variables.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(b) Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.

(c) Split the data into a training set and a test set.

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(d) Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(e) Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(f) Perform logistic regression on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(g) Perform naive Bayes on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~


(h) Perform KNN on the training data, with several values of K, in order to predict `mpg01`. Use only the variables that seemed most associated with `mpg01` in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?

Your code:
```{r,echo=TRUE}
#
#
```

Your answer:

~~~
Please write your answer in full sentences.


~~~

## Additional Material

### Classification using ML platforms

Using the COVID Data

```{r,echo=show_code}
size<-floor(dim(Covid_Data)[1]*0.3)
set.seed(1257)
samp_idx<-sample(1:dim(Covid_Data)[1],size,FALSE)
cols=c("SURVIVED","MEDICAL_UNIT","PATIENT_TYPE","PNEUMONIA","PREGNANT","DIABETES","COPD","ASTHMA","INMSUPR","HIPERTENSION","OTHER_DISEASE","CARDIOVASCULAR","OBESITY","RENAL_CHRONIC","TOBACCO","CLASIFFICATION_FINAL","FEMALE","AGE_TRUNK")
Train_COVID<- Covid_Data[samp_idx,cols]
Test_COVID <- Covid_Data[samp_idx,cols]

# "INTUBED","ICU",

# Not recommended, but for now.
train <- Train_COVID[complete.cases(Train_COVID),]
test<- Test_COVID[complete.cases(Test_COVID),]

```

#### Classification using MLR3

```{r}
# load packages and data
library(mlr3)
library(mlr3learners)

# fit a model
task <- as_task_classif(train, id = "covid", target = "SURVIVED")
learner <- lrn("classif.log_reg", predict_type = "prob")
learner$train(task)
```

#### Classification using Tidymodel

```{r}
# load packages and data
library(tidymodels)

# fit a model
rec <- recipe(SURVIVED ~ ., data = train) 

clf <- logistic_reg()

wflow <- workflow() %>%
         add_recipe(rec) %>%
         add_model(clf)

model <- wflow %>% fit(data = train)


```

#### Classification using Caret
```{r}
# load packages and data
library(caret)

# fit a model
trc= trainControl(method = "none")
model <- caret::train(SURVIVED ~ ., data = train,  trControl = trc, method = "bayesglm")
```



#### Classification using h2o

```{r,echo=show_code,eval=FALSE}
# load packages and data
library(h2o)
packageVersion("h2o")
localH2O <- h2o.init(nthreads = -1, max_mem_size="4g") # Starting H2O
```

Serving the data to H2O

```{r,echo=show_code,eval=FALSE}
train_hf <- as.h2o(train)
test_hf <- as.h2o(test)
```

Model fitting 

```{r h2o_fit_glm,echo=show_code,eval=FALSE}
logistic.fit = h2o.glm(y = "SURVIVED",                               #response variable 
                       x = cols[-1],  #predictor variables
                      training_frame = train_hf,                  #data
                      family = "binomial",lambda = 0)           #specify the dist. of y and penalty parameter: lambda
logistic.fit

prediction=predict(logistic.fit,newdata = test_hf)
#h2o.exportFile(prediction, "/tmp/pred.csv", force = TRUE) #export prediction result as a file
h2o.shutdown(prompt =F) 
```


## Advanced Content


### Classification

The classification was originally a subject of interest to computer science and, more recently, to machine learning.  In classification, we have a setting similar to a regression. The most significant difference is response $y_i$ is categorical, usually $\{0,1\}$ or $\{-1,1\}$.  The predictors in regression are called features.  Classification aims to learn a model that returns a response class from given accurate features.

For training stage, set of observations $(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\stackrel{iid}{\sim}p(x,y)$ are used to train classifier $f(x)$, similar to estimating $w$ in regression.  And in the testing stage, we observe a set of observations $(x,y)\stackrel{iid}{\sim}p(x,y)$, which come from the same distribution as the training data but are independent of the training dataset.  The hope is the learned $\hat{f}(x)$ is close to $y$, which is similar to prediction in regression.

#### Perceptron

Let's look at a historical classifier called perceptron (1943).  We observe data as

| observation | feature 1 | feature 2 | $\cdots$  | feature p | response |
|:-----------:|:---------:|:---------:|:---------:|:---------:|:--------:|
| 1           | $x_{11}$  | $x_{12}$  | $\cdots$  | $x_{1p}$  | $y_1$    |
| 2           | $x_{21}$  | $x_{22}$  | $\cdots$  | $x_{2p}$  | $y_2$    |
| $\vdots$    | $\vdots$  | $\vdots$  | $\ddots$  | $\vdots$  | $\vdots$ |
| n           | $x_{n1}$  | $x_{n2}$  | $\cdots$  | $x_{np}$  | $y_n$    |
| &nbsp;      | $X_{1}$   | $X_{2}$   | $\cdots$  | $X_{p}$   | $Y$      |

$$
X=\left[
\begin{array}{cccc}
X_{1}&X_{2}&\cdots &X_{p} \\
\end{array}
\right]
\verb|, |
X_i.=\left[1,x_{i1},x_{i2},\cdots,x_{ip}\right]
\verb|, and |
w=\left[
\begin{array}{c}
w_{0} \\
\vdots\\
w_{p} \\
\end{array}
\right]
$$

where $y_i\in \{+1,-1\}$, $\mathbf{x}_{i}$ is the $i$th row of the $X$ matrix concatenated with a vector of 1 with length n on the left side.  

The perceptron classifier is defined as
$$f(x)=sgn(w_0+w_1 x_1+\cdots+w_px_p)$$
where $sgn(y)$ is nonlinear transformation defined as 
\begin{eqnarray}
sgn(y_i)\left\{
\begin{array}{ll}
+1 & \mbox{ if }y_i\geq 0\\
-1 & \mbox{ if } y_i <0
\end{array}
\right.
\end{eqnarray} 

Note that $sgn()$ is a step function that returns one of two values.  If we use 0/1 instead of -1/1 and if we smooth out the transformation function using a sigmoidal function or the inverse logit function, we get logistic regression.

Like any regression, we can define training loss.  The training error is defined as the percentage of misclassified examples.  
$$
\frac{1}{n}\sum^n_{i=1}1_{y_i\neq sgn(\mathbf{x}_i\mathbf{w})}
$$
We aim to find weights $w$ so that this function is minimized.  But this loss function is not continuous, nor is it convex.  If you try to calculate the derivative, it's 0 for the most part.  So simple as this problem may seem, you cannot use gradient descent.  Rosenblatt initially used the following iterative algorithm to solve the problem.

1. Choose $w_{(0)}$ at random, choose the learning rate $\eta$
2. For every epoch (iteration) $t$ and sample $i$ update $w_t$ as
$$w_{(t+1)}=w_{(t)} −\eta \Delta w \mathbf{x}_i$$
where
\begin{eqnarray}
\Delta w \left\{
    \begin{array}{l}
    1 \mbox{ if } y_i=1 \mbox{ and } \hat{y}_i= −1\\
    -1 \mbox{ if } y_i  =-1 \mbox{ and } \hat{y}_i=1 
    \end{array}
    \right.
\end{eqnarray} 

3. Stop when $|w_{(t+1)}−w_{(t)} |<\epsilon$


#### Surrogate loss function

If we can't solve a challenging problem, we can approach it using simple approximation.
Since we know how to take the derivative of squared error loss, we can turn the perceptron problem into a linear regression problem.
$$
\frac{1}{n}\sum^n_{i=1}(y_i - \mathbf{x}_i^T\mathbf{w})^2
$$

which would give us the $\hat{\mathbf{w}}$ that minimizes MSE is our usual LS estimator.
$$\hat{\mathbf{w}}_{MSE}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}$$
But does it work?  Let's see an example.  Here, we have a simple two-class classification problem using the iris data.  The goal is to find a line that separates the two classes (setosa vs versicolor).  Setosa (blue) and versicolor (red) have distinctive features, so they are well separated by just looking at the petals.
```{r,echo=FALSE}
make.grid<-function(x,length.out=10){
  rx<-apply(x,2,range)
  xx<-apply(rx,2,function(rxx) seq(min(rxx),max(rxx),length.out=length.out))
  xgrid <- expand.grid(xx[,1], xx[,2])
  colnames(xgrid)<-colnames(x)
  return(xgrid)
}
y=2*(as.integer(iris[1:100,5])-1)-1
x=scale(as.matrix(iris[1:100,3:4]))
plot(x[,1],x[,2],xlab=names(iris)[3],ylab=names(iris)[4],
     col=rgb(1*(y==1),0,abs(1-1*(y==1)),alpha=1),
     main= "iris setosa vs versicolor")
```
We will code the Versicolor as 1 and Setosa as -1.  Given the data, we know how to fit a linear regression model

$$y_i=w_0+w_2\mbox{Petal.Width}_i+w_1\mbox{Petal.Length}_i$$
```{r}
fit0<-lm(y~Petal.Length +Petal.Width, data=data.frame(x,y) )
summary(fit0)
```
The result defines a surface in 3D space
$$y_i=0+0.25`\mbox{Petal.Width}_i+0.73\mbox{Petal.Length}_i$$
What does the result look like?  
```{r,echo=FALSE}
library(scatterplot3d)
fit0<-lm(y~Petal.Length +Petal.Width, data=data.frame(x,y) )
s3d <-scatterplot3d(x[,1],x[,2],y, pch=16, highlight.3d=TRUE,
                    xlab="Petal.Length",ylab="Petal.Width",
  type="p", main="3D Scatterplot",angle=65)
s3d$plane3d(fit0, col = rgb(0,0,1,0.7),lty = "dotted")
#s3d$plane3d(0,0,0, col = rgb(1,0,1,1),lty = "dotted")

```
All the versicolor points are above the plane and all the setosa points are below.

#### separating hyperpane

If we draw into the figure a line that this plane and a plane at 0 intersect, we get a separating hyperplane.
You can get them by solving for $f(x)=0$.  So in terms of Petal Length
$$\mbox{Petal.Width}= - \frac{w_0}{w_2} - \frac{w_1}{w_2} \mbox{Petal.Length}$$
```{r,echo=FALSE}
plot(x[,1],x[,2],xlab=names(iris)[3],ylab=names(iris)[4],
     col=rgb(1*(y==1),0,abs(1-1*(y==1)),alpha=1),
     main= "iris setosa vs versicolor")
cf0=coef(fit0)
abline(-cf0[1]/cf0[3],-cf0[2]/cf0[3])
```
The linear model can differentiate binary outcomes.  So why do we need anything else?  The problem is that the loss function does not align with our objective.  If you look at the example below, the green and red points are separated well with a blue line.  Therefore, you are pretty sure any value above 4 is green.  Fitting the regression without one large point (very sure point) gives you the correct decision boundary.  However, with the squared error, larger values play the role of leverage point distorting the decision boundary, which contradicts what we want.  We will return to how we will deal with this later in the semester.  But for now, let's use a loss function that is more suitable for the data.
```{r}
x=c(1,2,3,5,6,7,25)
y=c(-1,-1,-1,1,1,1,1)
par(mfrow=c(1,3))
plot(x,y,col=c(2,2,2,3,3,3,3)); abline(v=4,col="blue")
plot(x,y,col=c(2,2,2,3,3,3,"white"),main="without sure value");abline(lm(y~x,subset=x<10));abline(h=0,lty=2,col="grey89"); abline(v=4,col="blue")
plot(x,y,col=c(2,2,2,3,3,3,3),main="with sure value");abline(lm(y~x));abline(h=0,lty=2,col="grey89"); abline(v=-coef(lm(y~x))[1]/coef(lm(y~x))[2],col="blue")
```


### [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)

- [Logit function](https://en.wikipedia.org/wiki/Logit) is a mathematical function that transforms a probability ($\pi \in (0,1)$) into log [odds](https://en.wikipedia.org/wiki/Odds)
$$logit(\pi) = \log\left(\frac{\pi}{1-\pi}\right)$$

- Logistic regression is a regression model for binary outcomes $y_i\in \{0,1\}$, $i = 1,\dots n$.  If we let $\pi_i$ be the probability of $y_i=1$, the modeler's interest is understanding what factors affect this probability.  It is similar to linear regression, however, the big distinction is that $\pi_i$ is bounded between 0 and 1.  One can still fit a linear regression model; it’s just that all the estimates will be off beyond the boundaries.
- To avoid such an issue, rather than fitting a linear model on $\pi_i$ we can fit a linear model on $logit(\pi_i)$:
\begin{align}
logit(\pi_i) &= \mathbf{x}_i\boldsymbol{\beta}
\end{align}
because $logit(\pi_i)$ is unbounded, this is a much better transformation for applying a linear model.

- We can also denote the model using the inverse logit function as a nonlinear model on the probability of success $\pi_i$: 
$$P(y_i=1|\mathbf{x}_i) = \pi_i= logit^{-1}(\mathbf{x}_i\boldsymbol{\beta})=\frac{e^{\mathbf{x}_i\boldsymbol{\beta}}}{1+e^{\mathbf{x}_i\boldsymbol{\beta}}}=\frac{1}{1+e^{-\mathbf{x}_i\boldsymbol{\beta}}}$$

- Logistic regression coefficient $\boldsymbol{\beta}$ is often estimated using Maximum Likelihood.

----

#### Likelihood

- For a binary outcome, the natural choice of a probability distribution is the Bernoulli distribution.
\begin{align}
y_i &= Bernoulli(\pi_i)
\end{align}

- The likelihood for observation $i$ is defined as
$$f(y_i | \mathbf{x}_i,\boldsymbol{\beta})= \pi_i^{y_i} (1-\pi_i)^{(1-y_i)}$$ where
$$\pi_i=logit^{-1}(\mathbf{x}_i\boldsymbol{\beta})=\frac{\exp(\mathbf{x}_i\boldsymbol{\beta})}{1+\exp(\mathbf{x}_i\boldsymbol{\beta})}$$
- The log-likelihood is
\begin{align}
\log(f(y_i|\boldsymbol{\beta}))&=\log\left(\pi_i^{y_i} (1-\pi_i)^{(1-y_i)}\right)\\
&= \log\left(\pi_i^{y_i}\right)+\log\left( (1-\pi_i)^{(1-y_i)}\right)\\
&= y_i\log(\pi_i)+(1-y_i)\log( 1-\pi_i)\\
&= y_i\log\pi_i - y_i\log(1-\pi_i) +\log(1-\pi_i)
\end{align}


----

#### Joint log-likelihood

- Assuming the observations are iid the joint log-likelihood for $n$ observations is a product of the individual likelihoods, which is the sum of the log-likelihoods.
\begin{align}
l(\boldsymbol{\beta};\mathbf{y})&= \sum_i( y_i\log\pi_i - y_i\log(1-\pi_i) +\log(1-\pi_i))\\
&=\sum_i^N\left( y_i(\log\pi_i-\log(1-\pi_i)) + \log(1-\pi_i)\right)\\
&=\sum_i^N\left( y_i\left(\log\left(\frac{\pi_i}{\log(1-\pi_i)}\right)\right) + \log(1-\pi_i)\right)\\
&=\sum_i^N\left( y_i\left(\mathbf{x}_i\boldsymbol{\beta}\right) + \log\left(\frac{1}{1+\exp(\mathbf{x}_i\boldsymbol{\beta})}\right)\right)\\
&=\sum_i^N\left( y_i(\mathbf{x}_i\boldsymbol{\beta}) - \log\left(1+\exp\left(\mathbf{x}_i\boldsymbol{\beta}\right)\right)\right)\\
\end{align}

----

#### Model fitting

#### Calculating the first and second moments

- If we calculate the score function for $\beta_k$ by taking the derivative of the joint log-likelihood wrt $\beta_k$
\begin{align}
s_k(\boldsymbol{\beta})=\frac{\partial l(\boldsymbol{\beta};\mathbf{y})}{\partial \beta_k}&=\frac{\partial }{\partial \beta_k}\sum_i^N( y_i(\mathbf{x}_i\boldsymbol{\beta}) - \log(1+\exp(\mathbf{x}_i\boldsymbol{\beta})))\\
&=\sum_i^N \left( y_i x_{ik} - \frac{\exp(\mathbf{x}_i\boldsymbol{\beta})}{(1+\exp(\mathbf{x}_i\boldsymbol{\beta}))}x_{ik}\right)\\
&=\sum_i^N ( y_i x_{ik} - \pi_ix_{ik})=\sum_i^N x_{ik}( y_i  - \pi_i)\\
\end{align}
We can collect this into a vector
\begin{align}
s(\boldsymbol{\beta})=\left[
\begin{array}{c}
s_1(\boldsymbol{\beta})\\
s_2(\boldsymbol{\beta})\\
\vdots\\
s_p(\boldsymbol{\beta})
\end{array}
\right]=
\left[
\begin{array}{c}
\sum_i^N x_{i1}( y_i  - \pi_i)\\
\sum_i^N x_{i2}( y_i  - \pi_i)\\
\vdots\\
\sum_i^N x_{ip}( y_i  - \pi_i)
\end{array}
\right]=\sum_i^N \mathbf{x}_{i}( y_i  - \pi_i)=\sum_i^N s(\boldsymbol{\beta})_i
\end{align}
- Noting that $E(s(\boldsymbol{\beta})_i)=\mathbf{0}$ we can calculate the expected Fisher Information for observation $i$ as
$$F(\boldsymbol{\beta})_i=Cov(s(\boldsymbol{\beta})_i)=E(s(\boldsymbol{\beta})_is(\boldsymbol{\beta})_i^T)$$
\begin{align}
E(s(\boldsymbol{\beta})_is(\boldsymbol{\beta})_i^T)&=E\left(\mathbf{x}_{i}( y_i  - \pi_i)(\mathbf{x}_{i}( y_i  - \pi_i))^T\right)\\
&=E\left(\mathbf{x}_{i}\mathbf{x}_{i}^T( y_i  - \pi_i)^2\right)\\
&=\mathbf{x}_{i}\mathbf{x}_{i}^T E\left(( y_i  - \pi_i)^2\right)\\
&=\mathbf{x}_{i}\mathbf{x}_{i}^T Var\left( y_i\right)\\
&=\mathbf{x}_{i}\mathbf{x}_{i}^T \pi_i(1-\pi_i)
\end{align}
The Fisher Information for all $n$ observations is 
$$F(\boldsymbol{\beta})=\sum_i^nF(\boldsymbol{\beta})_i=\sum_i^n\mathbf{x}_{i}\mathbf{x}_{i}^T \pi_i(1-\pi_i)$$
<!-- \frac{\partial^2 l(\boldsymbol{\beta};\mathbf{y})}{\partial \beta_{k} \partial \beta_{k'}}  -->
<!-- &=-\sum_i^N \left( \frac{\exp(\mathbf{x}_i\boldsymbol{\beta})}{(1+\exp(\mathbf{x}_i\boldsymbol{\beta}))}\right)\\ -->
<!-- &=-\sum_i^N (x_{ik} \pi_i(1-\pi_i) x_{ik'})\\ -->
- We will not show it here, but the observed and the expected Fisher Information coincides for the logistic regression model.  This is not true in general.
- We can also collect the terms into a matrix by using 
$$\mathbf{y}
=
\left[
\begin{array}{c}
y_{1}\\
y_{2}\\
\vdots\\
y_n
\end{array}
\right]\mbox{, }
\boldsymbol{\mu}=\left[
\begin{array}{c}
\pi_{1}\\
\pi_{2}\\
\vdots\\
\pi_n
\end{array}
\right]\mbox{, }
\mathbf{X}
=
\left[
\begin{array}{c}
\mathbf{x}_{1}^T\\
\mathbf{x}_{2}^T\\
\vdots\\
\mathbf{x}_{n}^T
\end{array}
\right]\mbox{, }
\mathbf{W}
=
\left[
\begin{array}{ccc}
\pi_{1}(1-\pi_{1})&\cdots&&0\\
0&\pi_{2}(1-\pi_{2})&\cdots&0\\
\vdots&\ddots&&\vdots\\
0&\cdots&&\pi_{n}(1-\pi_{n})
\end{array}
\right]$$
We can express the score function as
$$
s(\boldsymbol{\beta})=\sum_i^N \mathbf{x}_{i}( y_i  - \pi_i)= \mathbf{X}^T( \mathbf{y}  - \boldsymbol{\mu})
$$
and the Fisher Information as
$$F(\boldsymbol{\beta})=\sum_i^n\mathbf{x}_{i}\mathbf{x}_{i}^T \pi_i(1-\pi_i)=\mathbf{X}^T\mathbf{W}\mathbf{X}$$
<!-- - The critical value will be maximum if this matrix is negative definite. -->
<!-- - Also, this forms a variance covariance matrix of the parameter estimates. -->

- We use the Newton–Raphson or Fisher scoring algorithm to get the maximum likelihood.


----

#### Newton-Raphson Method for logistic regression

- The details of the Newton-Raphson method for finding the maximum likelihood are as follows.  

1. You start with a guess of the parameters $\boldsymbol{\beta}^{(0)}$.
2. With every iteration $t$ update 
$$
\boldsymbol{\beta}^{(t+1)}=\boldsymbol{\beta}^{(t)}+\left[F\left(\boldsymbol{\beta}^{(t)}\right)\right]^{-1}s\left(\boldsymbol{\beta}^{(t)}\right)
$$

- If we substitute in 
$$s(\boldsymbol{\beta})=\mathbf{X}^{T}(\mathbf{y}-\boldsymbol{\mu})$$
where $\boldsymbol{\mu}$ be vector where $\mu_i = \pi_i$
- And 
$$F(\boldsymbol{\beta})=-\mathbf{X}^{T}\mathbf{W}\mathbf{X}$$
where $\mathbf{W}$ be a diagonal matrix where $w_{ii} = \pi_i(1-\pi_i)$.

- Therefore, the coefficient estimates at iteration $t$, $\boldsymbol{\beta}^{(t)}$ is updated to $\boldsymbol{\beta}^{(t+1)}$ using the following formula:
$$
\boldsymbol{\beta}^{(t+1)}=\boldsymbol{\beta}^{(t)}+[\mathbf{X}^{T}\mathbf{W}^{(t)}\mathbf{X}]^{-1}\mathbf{X}^{T}(\mathbf{y}-\boldsymbol{\mu}^{(t)})
$$
note here that $\boldsymbol{\mu}^{(t)}$ and $\mathbf{W}^{(t)}$ changes with the state of $\boldsymbol{\beta}^{(t)}$.


---- 

#### Newton-Raphson using R

- The Newton-Raphson algorithm for logistic regression can be implemented using just a couple of lines of code.
```{r}
NewtonRaphson<-function(y,desmat,beta_init_guess,maxiter=20){
  beta_guess <- matrix(NA,2,maxiter)
  beta_guess[,1]<- beta_init_guess 
  # main loop
  for( i in 1:19){
    pi_guess <- invlogit(desmat%*%beta_guess[,i])
    wii      <- as.vector(pi_guess*(1-pi_guess))
    lb       <- t(desmat)%*%(y-pi_guess)              # X'(y-p)
    llbinv   <- solve(t(desmat)%*%diag(wii)%*%desmat) # X'WX^-1
    beta_guess[,i+1] <- beta_guess[,i]+llbinv%*%lb
  }
  return(beta_guess)
}
```

```{r,echo=TRUE}
set.seed(123)
# Generate fake data
x<-rnorm(1000)
y<-rbinom(1000,1,prob=invlogit(1+x*3))
desmat <- cbind(1,x)
# Run the algorithm
beta_guess<-NewtonRaphson(y,desmat,beta_init_guess=c(0.1,0.1))
```

- Below, each point is an estimate at a particular iteration.  
```{r,echo=FALSE,fig.height=3,fig.width=3}
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
plot(1:20,beta_guess[2,],ylim=c(0,4),xlab="iteration",ylab="beta");abline(h=3,lty=2,col="red")
```

    As you can see, the algorithm starts from a guess and quickly approaches the actual value, which is expressed using a dashed horizontal red line.
    
- We can generate the data many times and run the algorithm repeatedly.  Below, we plot the trace of each algorithm.  Notice there that there is variability in the estimate due to sampling.
```{r, out.width="90%"}
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
set.seed(123)
kk = 20 
beta_guess_list  <- vector("list",kk)
niter=19
for(k in 1:kk){
  x<-rnorm(1000)
  y<-rbinom(1000,1,prob=invlogit(1+x*3))
  beta_guess_list[[k]]<-matrix(NA,2,20)
  beta_guess<- beta_guess_list[[k]]
  beta_guess[,1]<-  c(0.1,0.1)
  desmat <- cbind(1,x)
  for( i in 1:niter){
    pi_guess <- invlogit(desmat%*%beta_guess[,i])
    wii <- as.vector(pi_guess*(1-pi_guess))
    beta_guess[,i+1] <- beta_guess[,i]+solve(t(desmat)%*%diag(wii)%*%desmat)%*%(t(desmat)%*%(y-pi_guess))
  }
  beta_guess_list[[k]]<-beta_guess
}
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
plot(c(1,20),c(0,4),type="n",xlab="iteration",ylab="beta");abline(h=3,lty=2,col="red")
for(k in 1:kk) {lines(1:20,beta_guess_list[[k]][2,],col=rgb(0,0,0,alpha=0.3))}
```

#### Generative vs discriminative

Notice that this likelihood $\prod^n_{i=1}f(y_i=1\mid \mathbf{x}_i,\boldsymbol{\beta})$ is a likelihood WRT $\mathbf{y}$ which ignores $\mathbf{X}$s (or treats it as given) thus it is termed partial-likelihood.  In discriminative studies where the study focuses on the model's performance on tasks such as classification, this is the objective being used.  

On the other hand, where interest is understanding the mechanism that generated the data, we need a generative model where the interest is on modeling the full-likelihood as $\prod^n_{i=1}f(y_i=1,\mathbf{x}_i\mid \boldsymbol{\beta})$.     
Notice full likelihood can be factored as
$$
\prod^n_{i=1}f(y_i=1,\mathbf{x}_i\mid \boldsymbol{\beta})=\prod^n_{i=1}f(\mathbf{x}_i\mid \boldsymbol{\beta})f(y_i=1\mid \mathbf{x}_i,\boldsymbol{\beta})
$$
which is the partial-likelihood used in logistic regression  $\prod^n_{i=1}f(y_i=1\mid \mathbf{x}_i,\boldsymbol{\beta})$ times $f(\mathbf{x}_i\mid \boldsymbol{\beta})$ term.  This term can be thought of as observed data likelihood.  If there is information about $\boldsymbol{\beta}$ in the observed data, ignoring this term results in a loss of information.  Alternatively, if we factor the full likelihood as a generative model
$$
\prod^n_{i=1}p(y_i=1,\mathbf{x}_i\mid \boldsymbol{\beta})=\prod^n_{i=1}p(y_i=1\mid \boldsymbol{\beta})p(\mathbf{x}_i\mid y_i,\boldsymbol{\beta})
$$
Then you can think of $p(y_i=1\mid \boldsymbol{\beta})$ term as the prior distribution similar to Naive Bayes. 

The likelihood $f(\mathbf{x}_i\mid y_i,\boldsymbol{\beta})$ can be partitioned into two 
  \begin{eqnarray}
  f(\mathbf{x}_i\mid y_i,\beta)\left\{
    \begin{array}{l}
    f(\mathbf{x}_i\mid y_i=1,\beta)=p_{+}(X)\\
    f(\mathbf{x}_i\mid y_i=0,\beta)=p_{-}(X)
    \end{array}
    \right.
\end{eqnarray} 

$$
logit(\pi_i) = log\frac{f(y_i=1\mid \mathbf{x}_i, \boldsymbol{\beta})}{f(y_i=0\mid \mathbf{x}_i, \boldsymbol{\beta})}=log \frac{f(\mathbf{x}_i\mid y_i=1,\boldsymbol{\beta})f(y_i=1\mid \boldsymbol{\beta})}{f(\mathbf{x}_i\mid y_i=0,\boldsymbol{\beta})f(y_i=0\mid  \boldsymbol{\beta})}
$$
In logistic regression with a threshold of 0.5, we predict the outcome to be 1 if this quantity is positive.  In naive Bayes with two classes, we predict 1 when the right-hand side ratio exceeds 1. You can work out the details to show how they connect. Logistic regression and naive Bayes / normal discriminant analysis are referred to as a generative-discriminative pair. The predictive performances are comparable and depend on the data and how closely they match the model's assumptions. You can read more about it in Ng & Jordan (2002).

